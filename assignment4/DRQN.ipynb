{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Recurrent Q-Network \n",
    "This notebook provides an example implementation of a Deep Recurrent Q-Network which can solve Partially Observable Markov Decision Processes. To learn more about DRQNs, see my blog post on them here: https://medium.com/p/68463e9aeefc .\n",
    "\n",
    "For more reinforcment learning tutorials, as well as the additional required `gridworld.py` and `helper.py` see:\n",
    "https://github.com/awjuliani/DeepRL-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import tensorflow.contrib.slim as slim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gridworld import gameEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to adjust the size of the gridworld. Making it smaller (adjusting `size`) provides an easier task for our DRQN agent, while making the world larger increases the challenge.\n",
    "\n",
    "Initializing the Gridworld with `True` limits the field of view, resulting in a partially observable MDP. Initializing it with `False` provides the agent with the entire environment, resulting in a fully MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBl9q0P8CvEUYoCZW8nsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1119b7d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gameEnv(partial=False,size=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1119d25f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gameEnv(partial=True,size=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are examples of a starting environment in our simple game. The agent controls the blue square, and can move up, down, left, or right. The goal is to move to the green squares (for +1 reward) and avoid the red squares (for -1 reward). When the agent moves through a green or red square, it is randomly moved to a new place in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size,rnn_cell,myScope):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        self.conv1 = slim.convolution2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,\\\n",
    "            kernel_size=[8,8],stride=[4,4],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv1')\n",
    "        self.conv2 = slim.convolution2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,\\\n",
    "            kernel_size=[4,4],stride=[2,2],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv2')\n",
    "        self.conv3 = slim.convolution2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,\\\n",
    "            kernel_size=[3,3],stride=[1,1],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv3')\n",
    "        self.conv4 = slim.convolution2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,\\\n",
    "            kernel_size=[7,7],stride=[1,1],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv4')\n",
    "        \n",
    "        self.trainLength = tf.placeholder(dtype=tf.int32)\n",
    "        #We take the output from the final convolutional layer and send it to a recurrent layer.\n",
    "        #The input must be reshaped into [batch x trace x units] for rnn processing, \n",
    "        #and then returned to [batch x units] when sent through the upper levles.\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32,shape=[])\n",
    "        self.convFlat = tf.reshape(slim.flatten(self.conv4),[self.batch_size,self.trainLength,h_size])\n",
    "        self.state_in = rnn_cell.zero_state(self.batch_size, tf.float32)\n",
    "        self.rnn,self.rnn_state = tf.nn.dynamic_rnn(\\\n",
    "                inputs=self.convFlat,cell=rnn_cell,dtype=tf.float32,initial_state=self.state_in,scope=myScope+'_rnn')\n",
    "        self.rnn = tf.reshape(self.rnn,shape=[-1,h_size])\n",
    "        #The output from the recurrent player is then split into separate Value and Advantage streams\n",
    "        self.streamA,self.streamV = tf.split(self.rnn,2,1)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2,4]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        self.salience = tf.gradients(self.Advantage,self.imageIn)\n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,4,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        \n",
    "        #In order to only propogate accurate gradients through the network, we will mask the first\n",
    "        #half of the losses for each trace as per Lample & Chatlot 2016\n",
    "        self.maskA = tf.zeros([self.batch_size,self.trainLength//2])\n",
    "        self.maskB = tf.ones([self.batch_size,self.trainLength//2])\n",
    "        self.mask = tf.concat([self.maskA,self.maskB],1)\n",
    "        self.mask = tf.reshape(self.mask,[-1])\n",
    "        self.loss = tf.reduce_mean(self.td_error * self.mask)\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size,rnn_cell,myScope):\n",
    "        # This part is the same as in DQN_WorldNavigate\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        # Here we add the recurrent neurons into our network\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32,shape=[])\n",
    "        self.trainLength = tf.placeholder(dtype=tf.int32)\n",
    "        convFlat = tf.reshape(slim.flatten(self.conv4),[self.batch_size, self.trainLength, h_size])\n",
    "        \n",
    "        self.state_in = rnn_cell.zero_state(batch_size, tf.float32)\n",
    "        self.rnn, self.rnn_state = tf.nn.dynamic_rnn(\\\n",
    "                inputs=convFlat,cell=rnn_cell,dtype=tf.float32,initial_state=self.state_in,scope=myScope+'_rnn')\n",
    "        self.rnn = tf.reshape(self.rnn, shape=[-1,h_size])\n",
    "        \n",
    "        # Similarly, split into Value and Advantage weights\n",
    "        adv_branch, value_branch = tf.split(self.rnn, 2, 1)\n",
    "        adv_weight = tf.Variable(tf.random_normal([h_size//2,4]))\n",
    "        value_weight = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(adv_branch, adv_weight)\n",
    "        self.Value = tf.matmul(value_branch, value_weight)\n",
    "        \n",
    "        # Get our final Q-values\n",
    "        self.salience = tf.gradients(self.Advantage,self.imageIn)\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,4,dtype=tf.float32)\n",
    "        \n",
    "        Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        self.loss = tf.reduce_mean(tf.square(self.targetQ - Q))\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classes allow us to store experies and sample then randomly to train the network.\n",
    "Episode buffer stores experiences for each individal episode.\n",
    "Experience buffer stores entire episodes of experience, and sample() allows us to get training batches needed from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 1000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + 1 >= self.buffer_size:\n",
    "            self.buffer[0:(1+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self,batch_size,trace_length):\n",
    "        sampled_episodes = random.sample(self.buffer,batch_size)\n",
    "        sampledTraces = []\n",
    "        for episode in sampled_episodes:\n",
    "            point = np.random.randint(0,len(episode)+1-trace_length)\n",
    "            sampledTraces.append(episode[point:point+trace_length])\n",
    "        sampledTraces = np.array(sampledTraces)\n",
    "        return np.reshape(sampledTraces,[batch_size*trace_length,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These functions allows us to update the parameters of our target network with those of the primary network.\n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These training parameters are basically the same as DQN_WorldNavigate\n",
    "batch_size = 4 #How many experience traces to use for each training step.\n",
    "update_freq = 5 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 10000 #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 5000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./drqn\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001\n",
    "\n",
    "# Newly added params\n",
    "time_per_step = 1 #Length of each step used in gif creation\n",
    "summaryLength = 100 #Number of epidoes to periodically save for analysis\n",
    "trace_length = 8 #How long each experience trace will be when training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Set Success\n",
      "Episode 9 reward: 0.1\n",
      "Episode 19 reward: 0.3\n",
      "Episode 29 reward: 0.5\n",
      "Episode 39 reward: 1.1\n",
      "Episode 49 reward: 1.0\n",
      "Episode 59 reward: 0.6\n",
      "Episode 69 reward: 0.0\n",
      "Episode 79 reward: 1.0\n",
      "Episode 89 reward: 0.3\n",
      "Episode 99 reward: 0.1\n",
      "Episode 109 reward: 0.9\n",
      "Episode 119 reward: 0.4\n",
      "Episode 129 reward: 0.2\n",
      "Episode 139 reward: 0.7\n",
      "Episode 149 reward: 0.6\n",
      "Episode 159 reward: 1.3\n",
      "Episode 169 reward: 0.2\n",
      "Episode 179 reward: 0.5\n",
      "Episode 189 reward: 1.2\n",
      "Episode 199 reward: 0.9\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode 209 reward: 0.7\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-25e071618d84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0mtargetQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdoubleQ\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mend_multiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0;31m#Update the network with our target values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateModel\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalarInput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainLength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrace_length\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstate_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mrAll\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Here define the cells for the primary and target q-networks\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "cellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainQN = Qnetwork(h_size, cell,'main')\n",
    "targetQN = Qnetwork(h_size, cellT,'target')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path) \n",
    "  \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = []\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        state = (np.zeros([1,h_size]),np.zeros([1,h_size])) #Reset the recurrent layer's hidden state\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: \n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                state1 = sess.run(mainQN.rnn_state,\\\n",
    "                    feed_dict={mainQN.scalarInput:[s/255.0],mainQN.trainLength:1,mainQN.state_in:state,mainQN.batch_size:1})\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a, state1 = sess.run([mainQN.predict,mainQN.rnn_state],\\\n",
    "                    feed_dict={mainQN.scalarInput:[s/255.0],mainQN.trainLength:1,mainQN.state_in:state,mainQN.batch_size:1})\n",
    "                a = a[0]\n",
    "            s1,r,d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.append(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    updateTarget(targetOps,sess)\n",
    "                    #Reset the recurrent layer's hidden state\n",
    "                    state_train = (np.zeros([batch_size,h_size]),np.zeros([batch_size,h_size])) \n",
    "                    \n",
    "                    trainBatch = myBuffer.sample(batch_size,trace_length) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={\\\n",
    "                        mainQN.scalarInput:np.vstack(trainBatch[:,3]/255.0),\\\n",
    "                        mainQN.trainLength:trace_length,mainQN.state_in:state_train,mainQN.batch_size:batch_size})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={\\\n",
    "                        targetQN.scalarInput:np.vstack(trainBatch[:,3]/255.0),\\\n",
    "                        targetQN.trainLength:trace_length,targetQN.state_in:state_train,targetQN.batch_size:batch_size})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size*trace_length),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    #Update the network with our target values.\n",
    "                    sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]/255.0),mainQN.targetQ:targetQ,\\\n",
    "                        mainQN.actions:trainBatch[:,1],mainQN.trainLength:trace_length,\\\n",
    "                        mainQN.state_in:state_train,mainQN.batch_size:batch_size})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            sP = s1P\n",
    "            state = state1\n",
    "            if d == True:\n",
    "                break\n",
    "\n",
    "        #Add the episode to the experience buffer\n",
    "        episodeBuffer = list(zip(np.array(episodeBuffer)))\n",
    "        myBuffer.add(episodeBuffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "\n",
    "        #Periodically save the model. \n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print (\"Saved Model\")\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(\"Episode\",i,\"reward:\",np.mean(rList[-10:]))\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "\n",
    "print(\"Mean reward per episode: \" + str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Checking network learning\n",
    "Mean reward over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a36642cf8>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2d284400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
