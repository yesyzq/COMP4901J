{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Based RL\n",
    "In this exercise you will implement a policy and model network which work in tandem to solve the CartPole reinforcement learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a model and why would we want to use one? In this case, a model is going to be a neural network that attempts to learn the dynamics of the real environment. For example, in the CartPole we would like a model to be able to predict the next position of the Cart given the previous position and an action. By learning an accurate model, we can train our agent using the model rather than requiring to use the real environment every time. While this may seem less useful when the real environment is itself a simulation, like in our CartPole task, it can have huge advantages when attempting to learn policies for acting in the physical world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are we going to accomplish this in Tensorflow? We are going to be using a neural network that will learn the transition dynamics between a previous observation and action, and the expected new observation, reward, and done state. Our training procedure will involve switching between training our model using the real environment, and training our agentâ€™s policy using the model environment. By using this approach we will be able to learn a policy that allows our agent to solve the CartPole task without actually ever training the policy on the real environment! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries and starting CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info.major > 2:\n",
    "    xrange = range\n",
    "del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 8 # number of hidden layer neurons\n",
    "learning_rate = 1e-2\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "\n",
    "model_bs = 3 # Batch size when learning from model\n",
    "real_bs = 3 # Batch size when learning from real environment\n",
    "\n",
    "# model initialization\n",
    "D = 4 # input dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "observations = tf.placeholder(tf.float32, [None,4] , name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[4, H],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1,W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_grad1\")\n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W2Grad]\n",
    "\n",
    "################################################################################\n",
    "# TODO: Implement the loss function.                                           #\n",
    "# This sends the weights in the direction of making actions that gave good     #\n",
    "# advantage (reward overtime) more likely, and actions that didn't less likely.#\n",
    "################################################################################\n",
    "log_ = tf.log(input_y * (input_y - probability) + (1 - input_y) * (input_y + probability))  # think\n",
    "loss = -tf.reduce_mean(log_ * advantages) \n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Network\n",
    "Here we implement a multi-layer neural network that predicts the next observation, reward, and done state from a current state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mH = 256 # model layer size\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, [None, 5])\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [mH, 50])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [50])\n",
    "\n",
    "previous_state = tf.placeholder(tf.float32, [None,5] , name=\"previous_state\")\n",
    "W1M = tf.get_variable(\"W1M\", shape=[5, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1M = tf.Variable(tf.zeros([mH]),name=\"B1M\")\n",
    "layer1M = tf.nn.relu(tf.matmul(previous_state,W1M) + B1M)\n",
    "W2M = tf.get_variable(\"W2M\", shape=[mH, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2M = tf.Variable(tf.zeros([mH]),name=\"B2M\")\n",
    "layer2M = tf.nn.relu(tf.matmul(layer1M,W2M) + B2M)\n",
    "wO = tf.get_variable(\"wO\", shape=[mH, 4],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wR = tf.get_variable(\"wR\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wD = tf.get_variable(\"wD\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "bO = tf.Variable(tf.zeros([4]),name=\"bO\")\n",
    "bR = tf.Variable(tf.zeros([1]),name=\"bR\")\n",
    "bD = tf.Variable(tf.ones([1]),name=\"bD\")\n",
    "\n",
    "\n",
    "predicted_observation = tf.matmul(layer2M,wO,name=\"predicted_observation\") + bO\n",
    "predicted_reward = tf.matmul(layer2M,wR,name=\"predicted_reward\") + bR\n",
    "predicted_done = tf.sigmoid(tf.matmul(layer2M,wD,name=\"predicted_done\") + bD)\n",
    "\n",
    "true_observation = tf.placeholder(tf.float32,[None,4],name=\"true_observation\")\n",
    "true_reward = tf.placeholder(tf.float32,[None,1],name=\"true_reward\")\n",
    "true_done = tf.placeholder(tf.float32,[None,1],name=\"true_done\")\n",
    "\n",
    "\n",
    "predicted_state = tf.concat([predicted_observation,predicted_reward,predicted_done],1)\n",
    "\n",
    "observation_loss = tf.square(true_observation - predicted_observation)\n",
    "\n",
    "reward_loss = tf.square(true_reward - predicted_reward)\n",
    "\n",
    "done_loss = tf.multiply(predicted_done, true_done) + tf.multiply(1-predicted_done, 1-true_done)\n",
    "done_loss = -tf.log(done_loss)\n",
    "\n",
    "model_loss = tf.reduce_mean(observation_loss + done_loss + reward_loss)\n",
    "\n",
    "modelAdam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "updateModel = modelAdam.minimize(model_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resetGradBuffer(gradBuffer):\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    return gradBuffer\n",
    "\n",
    "def discount_rewards(r):\n",
    "    ################################################################################\n",
    "    # TODO: Implement the discounted rewards function                              #\n",
    "    # Return discounted rewards weighed by gamma. Each reward will be replaced     #\n",
    "    # with a weight reward that involves itself and all the other rewards occuring #\n",
    "    # after it. The later the reward after it happens, the less effect it has on   #\n",
    "    # the current rewards's discounted reward                                      #\n",
    "    # Hint: [r0, r1, r2, ..., r_N] will look someting like:                        #\n",
    "    #       [(r0 + r1*gamma^1 + ... r_N*gamma^N), (r1 + r2*gamma^1 + ...), ...]    #\n",
    "    ################################################################################\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r        \n",
    "    ################################################################################\n",
    "    #                                 END OF YOUR CODE                             #\n",
    "    ################################################################################\n",
    "\n",
    "# This function uses our model to produce a new state when given a previous state and action\n",
    "def stepModel(sess, xs, action):\n",
    "    toFeed = np.reshape(np.hstack([xs[-1][0],np.array(action)]),[1,5])\n",
    "    myPredict = sess.run([predicted_state],feed_dict={previous_state: toFeed})\n",
    "    reward = myPredict[0][:,4]\n",
    "    observation = myPredict[0][:,0:4]\n",
    "    observation[:,0] = np.clip(observation[:,0],-2.4,2.4)\n",
    "    observation[:,2] = np.clip(observation[:,2],-0.4,0.4)\n",
    "    doneP = np.clip(myPredict[0][:,5],0,1)\n",
    "    if doneP > 0.1 or len(xs)>= 300:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Policy and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 5.000000. Reward 27.750000. action: 0.000000. mean reward 27.750000.\n",
      "World Perf: Episode 9.000000. Reward 33.000000. action: 1.000000. mean reward 27.802500.\n",
      "World Perf: Episode 13.000000. Reward 13.250000. action: 1.000000. mean reward 27.656975.\n",
      "World Perf: Episode 17.000000. Reward 17.000000. action: 0.000000. mean reward 27.550405.\n",
      "World Perf: Episode 21.000000. Reward 33.750000. action: 1.000000. mean reward 27.612401.\n",
      "World Perf: Episode 25.000000. Reward 21.500000. action: 0.000000. mean reward 27.551277.\n",
      "World Perf: Episode 29.000000. Reward 20.250000. action: 1.000000. mean reward 27.478264.\n",
      "World Perf: Episode 33.000000. Reward 21.750000. action: 1.000000. mean reward 27.420982.\n",
      "World Perf: Episode 37.000000. Reward 27.500000. action: 1.000000. mean reward 27.421772.\n",
      "World Perf: Episode 41.000000. Reward 20.000000. action: 1.000000. mean reward 27.347554.\n",
      "World Perf: Episode 45.000000. Reward 23.500000. action: 1.000000. mean reward 27.309079.\n",
      "World Perf: Episode 49.000000. Reward 20.000000. action: 0.000000. mean reward 27.235988.\n",
      "World Perf: Episode 53.000000. Reward 26.250000. action: 0.000000. mean reward 27.226128.\n",
      "World Perf: Episode 57.000000. Reward 20.000000. action: 0.000000. mean reward 27.153867.\n",
      "World Perf: Episode 61.000000. Reward 17.500000. action: 0.000000. mean reward 27.057328.\n",
      "World Perf: Episode 65.000000. Reward 32.250000. action: 1.000000. mean reward 27.109255.\n",
      "World Perf: Episode 69.000000. Reward 13.500000. action: 0.000000. mean reward 26.973162.\n",
      "World Perf: Episode 73.000000. Reward 16.500000. action: 1.000000. mean reward 26.868431.\n",
      "World Perf: Episode 77.000000. Reward 13.000000. action: 1.000000. mean reward 26.729746.\n",
      "World Perf: Episode 81.000000. Reward 30.500000. action: 0.000000. mean reward 26.767449.\n",
      "World Perf: Episode 85.000000. Reward 19.500000. action: 1.000000. mean reward 26.694774.\n",
      "World Perf: Episode 89.000000. Reward 30.250000. action: 1.000000. mean reward 26.730327.\n",
      "World Perf: Episode 93.000000. Reward 17.500000. action: 0.000000. mean reward 26.638023.\n",
      "World Perf: Episode 97.000000. Reward 20.500000. action: 1.000000. mean reward 26.576643.\n",
      "World Perf: Episode 101.000000. Reward 21.750000. action: 1.000000. mean reward 26.528377.\n",
      "World Perf: Episode 105.000000. Reward 19.000000. action: 1.000000. mean reward 26.239695.\n",
      "World Perf: Episode 109.000000. Reward 25.750000. action: 0.000000. mean reward 26.059450.\n",
      "World Perf: Episode 113.000000. Reward 19.250000. action: 1.000000. mean reward 25.818316.\n",
      "World Perf: Episode 117.000000. Reward 24.000000. action: 1.000000. mean reward 25.611032.\n",
      "World Perf: Episode 121.000000. Reward 21.000000. action: 1.000000. mean reward 25.537827.\n",
      "World Perf: Episode 125.000000. Reward 19.500000. action: 0.000000. mean reward 25.307440.\n",
      "World Perf: Episode 129.000000. Reward 12.500000. action: 0.000000. mean reward 25.078558.\n",
      "World Perf: Episode 133.000000. Reward 12.250000. action: 0.000000. mean reward 24.936176.\n",
      "World Perf: Episode 137.000000. Reward 29.250000. action: 1.000000. mean reward 24.876842.\n",
      "World Perf: Episode 141.000000. Reward 22.250000. action: 1.000000. mean reward 24.808733.\n",
      "World Perf: Episode 145.000000. Reward 21.750000. action: 0.000000. mean reward 24.661079.\n",
      "World Perf: Episode 149.000000. Reward 17.750000. action: 1.000000. mean reward 24.618063.\n",
      "World Perf: Episode 153.000000. Reward 15.250000. action: 0.000000. mean reward 24.536913.\n",
      "World Perf: Episode 157.000000. Reward 14.250000. action: 1.000000. mean reward 24.385107.\n",
      "World Perf: Episode 161.000000. Reward 20.500000. action: 1.000000. mean reward 24.196102.\n",
      "World Perf: Episode 165.000000. Reward 18.000000. action: 0.000000. mean reward 24.007391.\n",
      "World Perf: Episode 169.000000. Reward 25.500000. action: 1.000000. mean reward 26.261045.\n",
      "World Perf: Episode 173.000000. Reward 21.250000. action: 1.000000. mean reward 26.124535.\n",
      "World Perf: Episode 177.000000. Reward 16.000000. action: 1.000000. mean reward 25.963251.\n",
      "World Perf: Episode 181.000000. Reward 22.500000. action: 0.000000. mean reward 25.854139.\n",
      "World Perf: Episode 185.000000. Reward 31.000000. action: 0.000000. mean reward 25.832237.\n",
      "World Perf: Episode 189.000000. Reward 16.750000. action: 0.000000. mean reward 25.560688.\n",
      "World Perf: Episode 193.000000. Reward 22.500000. action: 1.000000. mean reward 25.465155.\n",
      "World Perf: Episode 197.000000. Reward 21.750000. action: 0.000000. mean reward 25.638618.\n",
      "World Perf: Episode 201.000000. Reward 20.750000. action: 1.000000. mean reward 26.009518.\n",
      "World Perf: Episode 205.000000. Reward 31.250000. action: 1.000000. mean reward 26.140800.\n",
      "World Perf: Episode 209.000000. Reward 21.750000. action: 1.000000. mean reward 26.725399.\n",
      "World Perf: Episode 213.000000. Reward 17.000000. action: 1.000000. mean reward 26.814434.\n",
      "World Perf: Episode 217.000000. Reward 26.000000. action: 1.000000. mean reward 26.741547.\n",
      "World Perf: Episode 221.000000. Reward 18.500000. action: 0.000000. mean reward 26.611031.\n",
      "World Perf: Episode 225.000000. Reward 18.250000. action: 1.000000. mean reward 30.603907.\n",
      "World Perf: Episode 229.000000. Reward 23.750000. action: 0.000000. mean reward 30.393858.\n",
      "World Perf: Episode 233.000000. Reward 37.000000. action: 0.000000. mean reward 30.311302.\n",
      "World Perf: Episode 237.000000. Reward 19.000000. action: 0.000000. mean reward 29.983562.\n",
      "World Perf: Episode 241.000000. Reward 30.250000. action: 0.000000. mean reward 30.230253.\n",
      "World Perf: Episode 245.000000. Reward 15.500000. action: 1.000000. mean reward 30.374226.\n",
      "World Perf: Episode 249.000000. Reward 23.000000. action: 1.000000. mean reward 30.669502.\n",
      "World Perf: Episode 253.000000. Reward 14.750000. action: 0.000000. mean reward 33.935616.\n",
      "World Perf: Episode 257.000000. Reward 22.500000. action: 0.000000. mean reward 33.670979.\n",
      "World Perf: Episode 261.000000. Reward 20.750000. action: 0.000000. mean reward 33.562687.\n",
      "World Perf: Episode 265.000000. Reward 21.000000. action: 1.000000. mean reward 33.677334.\n",
      "World Perf: Episode 269.000000. Reward 36.000000. action: 1.000000. mean reward 33.795143.\n",
      "World Perf: Episode 273.000000. Reward 30.250000. action: 1.000000. mean reward 34.100403.\n",
      "World Perf: Episode 277.000000. Reward 19.000000. action: 0.000000. mean reward 34.803364.\n",
      "World Perf: Episode 281.000000. Reward 23.500000. action: 0.000000. mean reward 34.841652.\n",
      "World Perf: Episode 285.000000. Reward 23.250000. action: 1.000000. mean reward 34.543823.\n",
      "World Perf: Episode 289.000000. Reward 24.250000. action: 0.000000. mean reward 34.298973.\n",
      "World Perf: Episode 293.000000. Reward 25.750000. action: 0.000000. mean reward 34.025738.\n",
      "World Perf: Episode 297.000000. Reward 29.250000. action: 0.000000. mean reward 34.307362.\n",
      "World Perf: Episode 301.000000. Reward 21.750000. action: 1.000000. mean reward 34.176601.\n",
      "World Perf: Episode 305.000000. Reward 34.000000. action: 0.000000. mean reward 34.086414.\n",
      "World Perf: Episode 309.000000. Reward 38.750000. action: 0.000000. mean reward 33.955090.\n",
      "World Perf: Episode 313.000000. Reward 24.500000. action: 0.000000. mean reward 33.741833.\n",
      "World Perf: Episode 317.000000. Reward 27.250000. action: 0.000000. mean reward 34.039005.\n",
      "World Perf: Episode 321.000000. Reward 29.250000. action: 0.000000. mean reward 33.764431.\n",
      "World Perf: Episode 325.000000. Reward 39.750000. action: 1.000000. mean reward 33.689247.\n",
      "World Perf: Episode 329.000000. Reward 29.250000. action: 1.000000. mean reward 33.384258.\n",
      "World Perf: Episode 333.000000. Reward 24.000000. action: 1.000000. mean reward 33.037289.\n",
      "World Perf: Episode 337.000000. Reward 45.500000. action: 0.000000. mean reward 32.988361.\n",
      "World Perf: Episode 341.000000. Reward 44.250000. action: 1.000000. mean reward 32.946072.\n",
      "World Perf: Episode 345.000000. Reward 24.500000. action: 0.000000. mean reward 32.719345.\n",
      "World Perf: Episode 349.000000. Reward 24.500000. action: 0.000000. mean reward 32.524254.\n",
      "World Perf: Episode 353.000000. Reward 43.750000. action: 0.000000. mean reward 32.445786.\n",
      "World Perf: Episode 357.000000. Reward 41.000000. action: 1.000000. mean reward 32.279556.\n",
      "World Perf: Episode 361.000000. Reward 30.750000. action: 0.000000. mean reward 32.030277.\n",
      "World Perf: Episode 365.000000. Reward 38.750000. action: 1.000000. mean reward 31.937717.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 369.000000. Reward 41.000000. action: 0.000000. mean reward 31.885706.\n",
      "World Perf: Episode 373.000000. Reward 33.750000. action: 0.000000. mean reward 31.919867.\n",
      "World Perf: Episode 377.000000. Reward 34.250000. action: 0.000000. mean reward 31.896952.\n",
      "World Perf: Episode 381.000000. Reward 30.250000. action: 1.000000. mean reward 31.881712.\n",
      "World Perf: Episode 385.000000. Reward 43.000000. action: 0.000000. mean reward 31.958244.\n",
      "World Perf: Episode 389.000000. Reward 32.000000. action: 1.000000. mean reward 31.800058.\n",
      "World Perf: Episode 393.000000. Reward 39.500000. action: 1.000000. mean reward 31.675051.\n",
      "World Perf: Episode 397.000000. Reward 29.000000. action: 1.000000. mean reward 31.608810.\n",
      "World Perf: Episode 401.000000. Reward 26.500000. action: 0.000000. mean reward 31.403563.\n",
      "World Perf: Episode 405.000000. Reward 37.000000. action: 0.000000. mean reward 31.267315.\n",
      "World Perf: Episode 409.000000. Reward 16.250000. action: 1.000000. mean reward 30.950356.\n",
      "World Perf: Episode 413.000000. Reward 31.750000. action: 1.000000. mean reward 30.752239.\n",
      "World Perf: Episode 417.000000. Reward 54.250000. action: 0.000000. mean reward 30.856434.\n",
      "World Perf: Episode 421.000000. Reward 46.500000. action: 1.000000. mean reward 32.540573.\n",
      "World Perf: Episode 425.000000. Reward 37.250000. action: 0.000000. mean reward 32.893856.\n",
      "World Perf: Episode 429.000000. Reward 28.250000. action: 1.000000. mean reward 32.901833.\n",
      "World Perf: Episode 433.000000. Reward 36.000000. action: 1.000000. mean reward 32.704323.\n",
      "World Perf: Episode 437.000000. Reward 58.750000. action: 1.000000. mean reward 32.753433.\n",
      "World Perf: Episode 441.000000. Reward 27.500000. action: 0.000000. mean reward 32.544350.\n",
      "World Perf: Episode 445.000000. Reward 68.000000. action: 1.000000. mean reward 32.657837.\n",
      "World Perf: Episode 449.000000. Reward 38.000000. action: 1.000000. mean reward 36.197395.\n",
      "World Perf: Episode 453.000000. Reward 40.250000. action: 0.000000. mean reward 35.970184.\n",
      "World Perf: Episode 457.000000. Reward 51.250000. action: 0.000000. mean reward 35.840626.\n",
      "World Perf: Episode 461.000000. Reward 41.500000. action: 0.000000. mean reward 35.603611.\n",
      "World Perf: Episode 465.000000. Reward 33.000000. action: 0.000000. mean reward 35.307072.\n",
      "World Perf: Episode 469.000000. Reward 52.500000. action: 1.000000. mean reward 35.421116.\n",
      "World Perf: Episode 473.000000. Reward 68.500000. action: 0.000000. mean reward 35.585857.\n",
      "World Perf: Episode 477.000000. Reward 33.000000. action: 1.000000. mean reward 35.296009.\n",
      "World Perf: Episode 481.000000. Reward 39.250000. action: 0.000000. mean reward 35.038963.\n",
      "World Perf: Episode 485.000000. Reward 29.000000. action: 1.000000. mean reward 34.689045.\n",
      "World Perf: Episode 489.000000. Reward 52.250000. action: 0.000000. mean reward 34.796471.\n",
      "World Perf: Episode 493.000000. Reward 40.250000. action: 0.000000. mean reward 34.571774.\n",
      "World Perf: Episode 497.000000. Reward 40.750000. action: 0.000000. mean reward 34.376186.\n",
      "World Perf: Episode 501.000000. Reward 36.000000. action: 0.000000. mean reward 34.214989.\n",
      "World Perf: Episode 505.000000. Reward 39.000000. action: 0.000000. mean reward 34.080048.\n",
      "World Perf: Episode 509.000000. Reward 111.250000. action: 0.000000. mean reward 34.746056.\n",
      "World Perf: Episode 513.000000. Reward 44.500000. action: 0.000000. mean reward 34.597992.\n",
      "World Perf: Episode 517.000000. Reward 53.250000. action: 0.000000. mean reward 34.558670.\n",
      "World Perf: Episode 521.000000. Reward 69.500000. action: 0.000000. mean reward 34.828129.\n",
      "World Perf: Episode 525.000000. Reward 53.500000. action: 0.000000. mean reward 34.752865.\n",
      "World Perf: Episode 529.000000. Reward 61.250000. action: 1.000000. mean reward 34.833275.\n",
      "World Perf: Episode 533.000000. Reward 43.000000. action: 0.000000. mean reward 34.926456.\n",
      "World Perf: Episode 537.000000. Reward 54.250000. action: 0.000000. mean reward 35.059158.\n",
      "World Perf: Episode 541.000000. Reward 37.250000. action: 1.000000. mean reward 34.845688.\n",
      "World Perf: Episode 545.000000. Reward 44.500000. action: 0.000000. mean reward 34.736591.\n",
      "World Perf: Episode 549.000000. Reward 68.250000. action: 0.000000. mean reward 35.020027.\n",
      "World Perf: Episode 553.000000. Reward 23.250000. action: 0.000000. mean reward 34.758793.\n",
      "World Perf: Episode 557.000000. Reward 78.000000. action: 0.000000. mean reward 35.137161.\n",
      "World Perf: Episode 561.000000. Reward 38.500000. action: 1.000000. mean reward 34.934597.\n",
      "World Perf: Episode 565.000000. Reward 32.500000. action: 1.000000. mean reward 34.685989.\n",
      "World Perf: Episode 569.000000. Reward 29.500000. action: 0.000000. mean reward 34.633995.\n",
      "World Perf: Episode 573.000000. Reward 84.750000. action: 1.000000. mean reward 35.208752.\n",
      "World Perf: Episode 577.000000. Reward 58.000000. action: 0.000000. mean reward 35.864643.\n",
      "World Perf: Episode 581.000000. Reward 26.250000. action: 0.000000. mean reward 37.500565.\n",
      "World Perf: Episode 585.000000. Reward 125.250000. action: 1.000000. mean reward 38.249073.\n",
      "World Perf: Episode 589.000000. Reward 56.250000. action: 1.000000. mean reward 38.281464.\n",
      "World Perf: Episode 593.000000. Reward 46.000000. action: 0.000000. mean reward 38.319649.\n",
      "World Perf: Episode 597.000000. Reward 78.500000. action: 0.000000. mean reward 38.672852.\n",
      "World Perf: Episode 601.000000. Reward 71.500000. action: 1.000000. mean reward 39.188309.\n",
      "World Perf: Episode 605.000000. Reward 102.500000. action: 0.000000. mean reward 39.656677.\n",
      "World Perf: Episode 609.000000. Reward 91.750000. action: 1.000000. mean reward 43.234360.\n",
      "World Perf: Episode 613.000000. Reward 85.000000. action: 1.000000. mean reward 46.539368.\n",
      "World Perf: Episode 617.000000. Reward 93.250000. action: 0.000000. mean reward 46.630249.\n",
      "World Perf: Episode 621.000000. Reward 101.250000. action: 1.000000. mean reward 46.817360.\n",
      "World Perf: Episode 625.000000. Reward 44.000000. action: 0.000000. mean reward 49.282619.\n",
      "World Perf: Episode 629.000000. Reward 73.750000. action: 0.000000. mean reward 49.310593.\n",
      "World Perf: Episode 633.000000. Reward 116.500000. action: 0.000000. mean reward 49.763481.\n",
      "World Perf: Episode 637.000000. Reward 75.000000. action: 0.000000. mean reward 52.841885.\n",
      "World Perf: Episode 641.000000. Reward 84.500000. action: 0.000000. mean reward 52.800190.\n",
      "World Perf: Episode 645.000000. Reward 65.500000. action: 0.000000. mean reward 52.634766.\n",
      "World Perf: Episode 649.000000. Reward 74.500000. action: 1.000000. mean reward 52.523544.\n",
      "World Perf: Episode 653.000000. Reward 76.250000. action: 0.000000. mean reward 53.406658.\n",
      "World Perf: Episode 657.000000. Reward 46.750000. action: 0.000000. mean reward 53.112816.\n",
      "World Perf: Episode 661.000000. Reward 90.750000. action: 1.000000. mean reward 53.153637.\n",
      "World Perf: Episode 665.000000. Reward 65.250000. action: 1.000000. mean reward 52.995869.\n",
      "World Perf: Episode 669.000000. Reward 70.500000. action: 0.000000. mean reward 52.787426.\n",
      "World Perf: Episode 673.000000. Reward 79.000000. action: 0.000000. mean reward 53.066631.\n",
      "World Perf: Episode 677.000000. Reward 47.750000. action: 1.000000. mean reward 52.778839.\n",
      "World Perf: Episode 681.000000. Reward 69.250000. action: 1.000000. mean reward 52.610542.\n",
      "World Perf: Episode 685.000000. Reward 91.250000. action: 0.000000. mean reward 53.037987.\n",
      "World Perf: Episode 689.000000. Reward 94.500000. action: 0.000000. mean reward 53.363663.\n",
      "World Perf: Episode 693.000000. Reward 64.000000. action: 0.000000. mean reward 54.465015.\n",
      "World Perf: Episode 697.000000. Reward 68.000000. action: 1.000000. mean reward 55.050797.\n",
      "World Perf: Episode 701.000000. Reward 105.750000. action: 1.000000. mean reward 55.328045.\n",
      "World Perf: Episode 705.000000. Reward 70.250000. action: 1.000000. mean reward 56.217834.\n",
      "World Perf: Episode 709.000000. Reward 63.000000. action: 1.000000. mean reward 56.898518.\n",
      "World Perf: Episode 713.000000. Reward 63.500000. action: 0.000000. mean reward 57.264568.\n",
      "World Perf: Episode 717.000000. Reward 97.500000. action: 0.000000. mean reward 57.270935.\n",
      "World Perf: Episode 721.000000. Reward 115.750000. action: 0.000000. mean reward 57.605621.\n",
      "World Perf: Episode 725.000000. Reward 58.750000. action: 0.000000. mean reward 57.277737.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 729.000000. Reward 86.000000. action: 0.000000. mean reward 57.497047.\n",
      "World Perf: Episode 733.000000. Reward 85.000000. action: 1.000000. mean reward 57.513203.\n",
      "World Perf: Episode 737.000000. Reward 114.000000. action: 0.000000. mean reward 57.980385.\n",
      "World Perf: Episode 741.000000. Reward 117.000000. action: 0.000000. mean reward 58.186684.\n",
      "World Perf: Episode 745.000000. Reward 104.500000. action: 1.000000. mean reward 58.361492.\n",
      "World Perf: Episode 749.000000. Reward 83.000000. action: 1.000000. mean reward 58.410374.\n",
      "World Perf: Episode 753.000000. Reward 88.500000. action: 1.000000. mean reward 58.283871.\n",
      "World Perf: Episode 757.000000. Reward 80.500000. action: 1.000000. mean reward 58.383106.\n",
      "World Perf: Episode 761.000000. Reward 75.000000. action: 0.000000. mean reward 59.489883.\n",
      "World Perf: Episode 765.000000. Reward 41.250000. action: 0.000000. mean reward 59.075367.\n",
      "World Perf: Episode 769.000000. Reward 111.750000. action: 0.000000. mean reward 59.766407.\n",
      "World Perf: Episode 773.000000. Reward 169.500000. action: 0.000000. mean reward 62.662922.\n",
      "World Perf: Episode 777.000000. Reward 123.750000. action: 0.000000. mean reward 66.203438.\n",
      "World Perf: Episode 781.000000. Reward 84.000000. action: 0.000000. mean reward 65.956528.\n",
      "World Perf: Episode 785.000000. Reward 104.000000. action: 1.000000. mean reward 67.162392.\n",
      "World Perf: Episode 789.000000. Reward 104.250000. action: 0.000000. mean reward 68.224487.\n",
      "World Perf: Episode 793.000000. Reward 124.250000. action: 0.000000. mean reward 68.304970.\n",
      "World Perf: Episode 797.000000. Reward 101.250000. action: 0.000000. mean reward 68.609505.\n",
      "World Perf: Episode 801.000000. Reward 106.250000. action: 0.000000. mean reward 68.712784.\n",
      "World Perf: Episode 805.000000. Reward 136.250000. action: 0.000000. mean reward 69.009758.\n",
      "World Perf: Episode 809.000000. Reward 68.500000. action: 0.000000. mean reward 69.246292.\n",
      "World Perf: Episode 813.000000. Reward 97.500000. action: 0.000000. mean reward 69.246986.\n",
      "World Perf: Episode 817.000000. Reward 117.250000. action: 0.000000. mean reward 69.351875.\n",
      "World Perf: Episode 821.000000. Reward 145.500000. action: 1.000000. mean reward 69.757881.\n",
      "World Perf: Episode 825.000000. Reward 150.750000. action: 1.000000. mean reward 70.515480.\n",
      "World Perf: Episode 829.000000. Reward 140.500000. action: 1.000000. mean reward 70.851677.\n",
      "World Perf: Episode 833.000000. Reward 99.500000. action: 1.000000. mean reward 70.985107.\n",
      "World Perf: Episode 837.000000. Reward 121.750000. action: 0.000000. mean reward 71.408195.\n",
      "World Perf: Episode 841.000000. Reward 116.250000. action: 1.000000. mean reward 74.219910.\n",
      "World Perf: Episode 845.000000. Reward 84.500000. action: 0.000000. mean reward 75.991554.\n",
      "World Perf: Episode 849.000000. Reward 153.500000. action: 0.000000. mean reward 78.987854.\n",
      "World Perf: Episode 853.000000. Reward 92.500000. action: 1.000000. mean reward 81.706657.\n",
      "World Perf: Episode 857.000000. Reward 151.250000. action: 0.000000. mean reward 81.776161.\n",
      "World Perf: Episode 861.000000. Reward 122.250000. action: 1.000000. mean reward 81.723473.\n",
      "World Perf: Episode 865.000000. Reward 184.000000. action: 0.000000. mean reward 82.222183.\n",
      "World Perf: Episode 869.000000. Reward 171.250000. action: 0.000000. mean reward 84.478416.\n",
      "World Perf: Episode 873.000000. Reward 158.000000. action: 0.000000. mean reward 84.579674.\n",
      "World Perf: Episode 877.000000. Reward 158.000000. action: 1.000000. mean reward 84.546814.\n",
      "World Perf: Episode 881.000000. Reward 132.500000. action: 1.000000. mean reward 84.533592.\n",
      "World Perf: Episode 885.000000. Reward 154.250000. action: 0.000000. mean reward 84.698288.\n",
      "World Perf: Episode 889.000000. Reward 165.500000. action: 0.000000. mean reward 84.783829.\n",
      "World Perf: Episode 893.000000. Reward 106.000000. action: 0.000000. mean reward 84.621521.\n",
      "World Perf: Episode 897.000000. Reward 178.750000. action: 1.000000. mean reward 85.117317.\n",
      "World Perf: Episode 901.000000. Reward 160.750000. action: 1.000000. mean reward 85.375008.\n",
      "World Perf: Episode 905.000000. Reward 150.500000. action: 1.000000. mean reward 85.867943.\n",
      "World Perf: Episode 909.000000. Reward 177.250000. action: 0.000000. mean reward 86.938110.\n",
      "World Perf: Episode 913.000000. Reward 189.250000. action: 1.000000. mean reward 87.966499.\n",
      "World Perf: Episode 917.000000. Reward 108.750000. action: 0.000000. mean reward 89.001945.\n",
      "World Perf: Episode 921.000000. Reward 173.500000. action: 0.000000. mean reward 91.022209.\n",
      "World Perf: Episode 925.000000. Reward 167.750000. action: 1.000000. mean reward 94.161697.\n",
      "World Perf: Episode 929.000000. Reward 192.000000. action: 0.000000. mean reward 96.828644.\n",
      "World Perf: Episode 933.000000. Reward 161.500000. action: 0.000000. mean reward 96.629265.\n",
      "World Perf: Episode 937.000000. Reward 174.750000. action: 0.000000. mean reward 96.569893.\n",
      "World Perf: Episode 941.000000. Reward 194.000000. action: 0.000000. mean reward 96.789871.\n",
      "World Perf: Episode 945.000000. Reward 200.000000. action: 0.000000. mean reward 97.058929.\n",
      "World Perf: Episode 949.000000. Reward 172.750000. action: 1.000000. mean reward 99.705338.\n",
      "World Perf: Episode 953.000000. Reward 139.500000. action: 0.000000. mean reward 100.663338.\n",
      "World Perf: Episode 957.000000. Reward 169.750000. action: 0.000000. mean reward 101.406265.\n",
      "World Perf: Episode 961.000000. Reward 185.000000. action: 0.000000. mean reward 101.460434.\n",
      "World Perf: Episode 965.000000. Reward 147.750000. action: 0.000000. mean reward 101.301811.\n",
      "World Perf: Episode 969.000000. Reward 199.250000. action: 0.000000. mean reward 102.773750.\n",
      "World Perf: Episode 973.000000. Reward 178.750000. action: 0.000000. mean reward 103.010490.\n",
      "World Perf: Episode 977.000000. Reward 172.250000. action: 1.000000. mean reward 103.040482.\n",
      "World Perf: Episode 981.000000. Reward 163.000000. action: 0.000000. mean reward 105.027855.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-21977bc98535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Start displaying environment once performance is acceptably high.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward_sum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m150\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdrawFromModel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrendering\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mrendering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pyglet/window/cocoa/__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_mouse_cursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pyglet/gl/cocoa.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nscontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflushBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;34m\"\"\"Call the method with the given arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;31m######################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, objc_id, *args)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m             \u001b[0;31m# Convert result to python type if it is a instance or class pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mObjCInstance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xs,drs,ys,ds = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "real_episodes = 1\n",
    "init = tf.global_variables_initializer()\n",
    "batch_size = real_bs\n",
    "\n",
    "drawFromModel = False # When set to True, will use model for observations\n",
    "trainTheModel = True # Whether to train the model\n",
    "trainThePolicy = False # Whether to train the policy\n",
    "switch_point = 1\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()\n",
    "    x = observation\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "    \n",
    "    while episode_number <= 5000:\n",
    "        # Start displaying environment once performance is acceptably high.\n",
    "        if (reward_sum/batch_size > 150 and drawFromModel == False) or rendering == True : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        x = np.reshape(observation,[1,4])\n",
    "\n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "\n",
    "        # record various intermediates (needed later for backprop)\n",
    "        xs.append(x) \n",
    "        y = 1 if action == 0 else 0 \n",
    "        ys.append(y)\n",
    "        \n",
    "        # step the  model or real environment and get new measurements\n",
    "        if drawFromModel == False:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "        else:\n",
    "            observation, reward, done = stepModel(sess,xs,action)\n",
    "                \n",
    "        reward_sum += reward\n",
    "        \n",
    "        ds.append(done*1)\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: \n",
    "            \n",
    "            if drawFromModel == False: \n",
    "                real_episodes += 1\n",
    "            episode_number += 1\n",
    "\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            epd = np.vstack(ds)\n",
    "            xs,drs,ys,ds = [],[],[],[] # reset array memory\n",
    "            \n",
    "            if trainTheModel == True:\n",
    "                \n",
    "                ################################################################################\n",
    "                # TODO: Run the model network and compute predicted_state                      #\n",
    "                # Output: 'pState'                                                             #\n",
    "                ################################################################################\n",
    "                actions = np.array([np.abs(y-1) for y in epy][:-1])\n",
    "                state_prevs = epx[:-1,:]\n",
    "                state_prevs = np.hstack([state_prevs,actions])\n",
    "                state_nexts = epx[1:,:]\n",
    "                rewards = np.array(epr[1:,:])\n",
    "                dones = np.array(epd[1:,:])\n",
    "                state_nextsAll = np.hstack([state_nexts,rewards,dones])\n",
    "\n",
    "                feed_dict={previous_state: state_prevs, true_observation: state_nexts,true_done:dones,true_reward:rewards}\n",
    "                loss,pState,_ = sess.run([model_loss,predicted_state,updateModel],feed_dict)\n",
    "                ################################################################################\n",
    "                #                                 END OF YOUR CODE                             #\n",
    "                ################################################################################\n",
    "                \n",
    "\n",
    "            if trainThePolicy == True:\n",
    "                \n",
    "                ################################################################################\n",
    "                # TODO: Run the policy network and compute newGrads                            #\n",
    "                # Output: 'tGrad'                                                              #\n",
    "                ################################################################################\n",
    "                discounted_epr = discount_rewards(epr).astype('float32')\n",
    "                discounted_epr -= np.mean(discounted_epr)\n",
    "                discounted_epr /= np.std(discounted_epr)\n",
    "                tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "                ################################################################################\n",
    "                #                                 END OF YOUR CODE                             #\n",
    "                ################################################################################\n",
    "                \n",
    "                # If gradients becom too large, end training process\n",
    "                if np.sum(tGrad[0] == tGrad[0]) == 0:\n",
    "                    break\n",
    "                for ix,grad in enumerate(tGrad):\n",
    "                    gradBuffer[ix] += grad\n",
    "                \n",
    "            if switch_point + batch_size == episode_number: \n",
    "                switch_point = episode_number\n",
    "                if trainThePolicy == True:\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    # TODO:                                                                        #\n",
    "                    # (1) Run the policy network and update gradients                              #\n",
    "                    # (2) Reset gradBuffer to 0                                                    #\n",
    "                    ################################################################################\n",
    "                    sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                    gradBuffer = resetGradBuffer(gradBuffer)                    \n",
    "                    ################################################################################\n",
    "                    #                                 END OF YOUR CODE                             #\n",
    "                    ################################################################################\n",
    "\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                if drawFromModel == False:\n",
    "                    print('World Perf: Episode %f. Reward %f. action: %f. mean reward %f.' % (real_episodes,reward_sum/real_bs,action, running_reward/real_bs))\n",
    "                    if reward_sum/batch_size > 200:\n",
    "                        break\n",
    "                reward_sum = 0\n",
    "\n",
    "                # Once the model has been trained on 100 episodes\n",
    "                if episode_number > 100:\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    # TODO: Alternating between training the policy from the model and training    #\n",
    "                    # the model from the real environment.                                         #\n",
    "                    ################################################################################\n",
    "                    drawFromModel = not drawFromModel\n",
    "                    trainTheModel = not trainTheModel\n",
    "                    trainThePolicy = not trainThePolicy\n",
    "                    ################################################################################\n",
    "                    #                                 END OF YOUR CODE                             #\n",
    "                    ################################################################################\n",
    "            \n",
    "            if drawFromModel == True:\n",
    "                observation = np.random.uniform(-0.1,0.1,[4]) # Generate reasonable starting point\n",
    "                batch_size = model_bs\n",
    "            else:\n",
    "                observation = env.reset()\n",
    "                batch_size = real_bs\n",
    "                \n",
    "print(real_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking model representation\n",
    "Here we can examine how well the model is able to approximate the true environment after training. The green line indicates the real environment, and the blue indicates model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(6):\n",
    "    plt.subplot(6, 2, 2*i + 1)\n",
    "    plt.plot(pState[:,i])\n",
    "    plt.subplot(6,2,2*i+1)\n",
    "    plt.plot(state_nextsAll[:,i])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra credit\n",
    "Since we have two networks involved in Q4, there are plenty of hyper-parameters to adjust in order to improve performance or efficiency. You are encouraged to play with them in order to discover better means of combining the models and get better results.\n",
    "\n",
    "** Description: **By adjusting these hyper-parameters, we are able to improve efficiency throughout the course of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = 10\n",
    "learning_rate = 7e-3\n",
    "gamma = 0.95\n",
    "decay_rate = 0.95\n",
    "resume = False\n",
    "model_bs = 5\n",
    "real_bs = 4\n",
    "D = 4\n",
    "\n",
    "# Then run the \"Training the Policy and Model\" cell above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before tuning the hyper-parameter, we are able to achieve meam reward 80 in around 1000 episodes.\n",
    "\n",
    "World Perf: Episode 1033.000000. Reward 132.000000. action: 1.000000. mean reward 80.877937.\n",
    "\n",
    "#### After tuning the hyper-parameter, the mean reward can reach ~80 within 800 episodes, which boosts efficiency for around 20%.\n",
    "\n",
    "World Perf: Episode 853.000000. Reward 92.500000. action: 1.000000. mean reward 81.706657."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
